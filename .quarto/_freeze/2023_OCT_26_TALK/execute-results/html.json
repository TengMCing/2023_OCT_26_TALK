{
  "hash": "40824a512bf7fbb88843eb0b5a753358",
  "result": {
    "markdown": "---\nformat: \n  revealjs:\n    transition-speed: fast\n    slide-number: c/t\n    css: custom.css\n    transition: fade\n    incremental: true \n    theme: default\n    footer: <https://patrick-lrd.netlify.app>\n    logo: figures/monash-stacked-blue-rgb.png\n---\n\n\n\n## A plot is worth a thousand tests: assessing residual diagnostics with the lineup {.center}\n\n#### Numbats seminar presentation\n\nWeihao (Patrick) Li\n\n---\n\n## {.center}\n\nLi et al. (2023) A Plot is Worth a Thousand Tests: Assessing Residual Diagnostics with the Lineup Protocol . arXiv preprint. URL: http://arxiv.org/abs/2308.05964\n\n\n---\n\n## üîçRegression Diagnostics\n\n\n\nDiagnostics are the key to determining whether there is anything **importantly wrong** with a regression model. \n\n<br>\n\n$$\\textrm{Residuals} = \\underbrace{\\boldsymbol{y}}_\\textrm{Observations} - \\underbrace{f(\\boldsymbol{x})}_\\textrm{Fitted values}$$\n\n**Residuals** summarise what is **not captured by the regression model**.\n\n---\n\n## üîçRegression Diagnostics\n\n\nResiduals can be checked in multiple ways:\n\n- **Numerical summaries**: variance, skewness, quantiles, etc.\n- **Statistical tests**: F-test, BP test, etc.  \n- **Diagnostic plots**: residual plots, Q-Q plots, etc.\n\n---\n\n## üìúLiteature of Regression Diagnostics\n\nGraphical approaches (plots) are the recommended methods for diagnosing model fits.\n\n::: {style=\"font-size:55%\"}\n\n::: {.fragment}\n::: {.nonincremental}\n- Draper and Smith (1998) and Belsley, Kuh, and Welsch (1980):\n:::\n\n> Residual plots are usually **revealing** when the assumptions are violated.\n:::\n\n::: {.fragment}\n::: {.nonincremental}\n- Cook and Weisberg (1982):\n:::\n\n> Formal tests and graphical procedures are **complementary** and both have a place in residual analysis, but **graphical methods are easier to use**.\n:::\n\n\n::: {.fragment}\n::: {.nonincremental}\n- Montgomery and Peck (1982):\n:::\n\n> **Residual plots are more informative in most practical situations** than the corresponding conventional hypothesis tests.\n:::\n\n:::\n\n---\n\n## üîçObjectives\n\nRegression experts **consistently recommend plotting residuals for regression diagnostics**, despite the existence of numerous hypothesis test procedures. \n\nOur study aims to **provide evidence** for this practice by utilizing data from a **visual inference experiment**, highlighting how visual inference enhances the reliability and consistency of residual plot interpretation in regression diagnostics.\n\n\n---\n\n## ü§îChallenges in Interpreting Residual Plots\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2023_OCT_26_TALK_files/figure-revealjs/unnamed-chunk-1-1.png){width=480}\n:::\n:::\n\n\n:::\n\n\n::: {.column width=\"60%\"}\n\nWhat do you observe from this residual plot?\n\n- Vertical spread of the points varies with the fitted values.\n- This often indicates **the existence of heteroskedasticity**.\n\n:::\n\n::::\n\n---\n\n## ü§îChallenges in Interpreting Residual Plots\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2023_OCT_26_TALK_files/figure-revealjs/unnamed-chunk-2-1.png){width=480}\n:::\n:::\n\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n- However, this is an **over-interpretation**.\n\n- The fitted model is **correctly specified**!\n\n- The triangle shape is caused by the **skewed distribution of the regressors**.\n\n:::\n\n::::\n\n---\n\n## We need to use an **inferential framework** to **calibrate** the reading of residual plots! {.center}\n\n\n---\n\n## üî¨Visual Inference\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2023_OCT_26_TALK_files/figure-revealjs/unnamed-chunk-3-1.png){width=480}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\nIf we\n\n- **simulate residuals** from the fitted model,\n\n- and embed the real residual plot in a matrix of simulated residual plots,\n\n- can you now find the **real residual plot**?\n\n:::\n\n\n::::\n\n---\n\n\n## üî¨Visual Inference\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2023_OCT_26_TALK_files/figure-revealjs/unnamed-chunk-4-1.png){width=480}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n- Probably not. (It is plot No.11)\n\n- It is **not uncommon** for residual plots of this model to exhibit a triangle shape.\n\n- **The visual discovery is calibrated via comparison**.\n\n\n\n:::\n\n\n::::\n\n\n---\n\n## üî¨Visual Inference\n\nThis framework is called **visual inference** (Buja, et al. 2009).\n\nA **lineup** consists of \n\n- $m$ randomly placed plots\n- one plot is the **data plot** \n- remaining $m ‚àí 1$ plots (**null plots**) containing data **consistent with the null hypothesis**.\n\n::: {.fragment}\n\nTo perform a **visual test**, observer(s) will be asked to select the **most different plot(s)** from the lineup. And we check if the data plot is identified.\n\n:::\n\n---\n\n## üé≤Simulate Residuals from the Fitted Model\n\nFor **classical normal linear regression model**, the **residual rotation technique** (Buja, et al. 2009) can be applied:\n\n```{.r code-line-numbers=\"2\"}\nfitted_mod <- lm(y ~ x1 + x2 + ... + xp, data = dat)\nw <- rnorm(n, mean = 0, sd = 1)\nnew_mod <- lm(w ~ x1 + x2 + ... + xp, data = dat)\nrotated_residuals <- resid(new_mod) * sqrt(rss(fitted_mod)/rss(new_mod))\n```\n\n::: {.nonincremental}\n1. Generate $w_i, i=1,...,n$ from $N(0, 1)$.\n:::\n\n---\n\n## üé≤Simulate Residuals from the Fitted Model\n\nFor **classical normal linear regression model**, the **residual rotation technique** (Buja, et al. 2009) can be applied:\n\n```{.r code-line-numbers=\"3\"}\nfitted_mod <- lm(y ~ x1 + x2 + ... + xp, data = dat)\nw <- rnorm(n, mean = 0, sd = 1)\nnew_mod <- lm(w ~ x1 + x2 + ... + xp, data = dat)\nrotated_residuals <- resid(new_mod) * sqrt(rss(fitted_mod)/rss(new_mod))\n```\n\n::: {.nonincremental}\n1. Generate $w_i, i=1,...,n$ from $N(0, 1)$.\n2. Regress $\\boldsymbol{w}$ on $\\boldsymbol{x}$ and obtain the residuals $r_i, i=1,...,n$.\n:::\n\n---\n\n## üé≤Simulate Residuals from the Fitted Model\n\nFor **classical normal linear regression model**, the **residual rotation technique** (Buja, et al. 2009) can be applied:\n\n```{.r code-line-numbers=\"4\"}\nfitted_mod <- lm(y ~ x1 + x2 + ... + xp, data = dat)\nw <- rnorm(n, mean = 0, sd = 1)\nnew_mod <- lm(w ~ x1 + x2 + ... + xp, data = dat)\nrotated_residuals <- resid(new_mod) * sqrt(rss(fitted_mod)/rss(new_mod))\n```\n\n::: {.nonincremental}\n1. Generate $w_i, i=1,...,n$ from $N(0, 1)$.\n2. Regress $\\boldsymbol{w}$ on $\\boldsymbol{x}$ and obtain the residuals $r_i, i=1,...,n$.\n3. Rescale $r_i$ by $\\sqrt{RSS_{fitted}/RSS_{new}}$ .\n:::\n\n---\n\n## üìàStatistical Significance\n\nThe p-value can be calculated using the **beta-binomial model** proposed in VanderPlas et al. (2021).\n\nGiven $c_i$, the **number of observers identify the data plot**,\n\n$$P(C \\geq c_i) = \\sum_{x=c_i}^{K}{K \\choose x}\\frac{B(x + \\alpha, K - x + (m - 1)\\alpha)}{B(\\alpha, (m-1)\\alpha)},\\text{ for } c_i \\in \\mathbb{Z}_0^+,$$\n\nwhere $B(.)$ is the beta function, $\\alpha$ is the parameter of the Dirichlet distribution reflecting the attractiveness of each plot in a lineup, and $K$ is the total number of observers. \n\n---\n\n## üß™Experimental Design\n\nTo understand the reasons behind the consistent recommendation of residual plots,\nan experiment is conducted to explore the differences between **conventional hypothesis testing** and **visual testing** in the application of **linear regression diagnostics**.\n\n::: {.fragment}\nWe focus on two types of residual departures\n\n- **Non-linearity**\n- **Heteroskedasticity**\n:::\n\n---\n\n## üß™Experimental Design\n\n### **Non-linearity model:**\n\n$$\\boldsymbol{y} = \\boldsymbol{1}_n + \\boldsymbol{x} + \\boldsymbol{z} + \\boldsymbol{\\varepsilon},~ \\boldsymbol{z} \\propto He_j(\\boldsymbol{x}) \\text{ and } \\boldsymbol{\\varepsilon} \\sim N(\\boldsymbol{0}_n, \\sigma^2\\boldsymbol{I}_n),$$\n\n\\noindent where $\\boldsymbol{y}$, $\\boldsymbol{x}$, $\\boldsymbol{\\varepsilon}$ are vectors of size $n$, $\\boldsymbol{1}_n$ is a vector of ones of size $n$, and $He_{j}(.)$ is the $j$th-order probabilist's Hermite polynomials.\n\n### **Null regression model:**\n\n$$\\boldsymbol{y} = \\beta_0 + \\beta_1\\boldsymbol{x} + \\boldsymbol{u}, ~\\boldsymbol{u} \\sim N(\\boldsymbol{0}_n, \\sigma^2\\boldsymbol{I}_n).$$\n\n---\n\n## üß™Experimental Design\n\n### **Heteroskedasticity model:**\n\n$$\\boldsymbol{y} = 1 + \\boldsymbol{x} + \\boldsymbol{\\varepsilon},~ \\boldsymbol{\\varepsilon} \\sim N(\\boldsymbol{0}, 1 + (2 - |a|)(\\boldsymbol{x} - a)^2b \\boldsymbol{I}),$$\n\n\\noindent where $\\boldsymbol{y}$, $\\boldsymbol{x}$, $\\boldsymbol{\\varepsilon}$ are vectors of size $n$, and $\\boldsymbol{1}_n$ is a vector of ones of size $n$.\n\n### **Null regression model:**\n\n$$\\boldsymbol{y} = \\beta_0 + \\beta_1\\boldsymbol{x} + \\boldsymbol{u}, ~\\boldsymbol{u} \\sim N(\\boldsymbol{0}_n, \\sigma^2\\boldsymbol{I}_n).$$\n\n---\n\n## üß™Experimental Design\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](2023_OCT_26_TALK_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=1200}\n:::\n\n::: {.cell-output-display}\n![](2023_OCT_26_TALK_files/figure-revealjs/unnamed-chunk-5-2.png){fig-align='center' width=1200}\n:::\n:::\n\n\n---\n\n## üß™Experimental Design\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](2023_OCT_26_TALK_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=864}\n:::\n\n::: {.cell-output-display}\n![](2023_OCT_26_TALK_files/figure-revealjs/unnamed-chunk-6-2.png){fig-align='center' width=1200}\n:::\n:::\n\n\n---\n\n## üß™Experimental Design\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](2023_OCT_26_TALK_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](2023_OCT_26_TALK_files/figure-revealjs/unnamed-chunk-7-2.png){fig-align='center' width=80%}\n:::\n:::\n\n\n---\n\n## üìèEffect size\n\n\nWe have chosen to use an approach based on **Kullback-Leibler divergence** (Kullback and Leibler, 1951).\n\nThe effect size of the **non-linearity model** is\n\n\\begin{align*}\nE &= \\frac{1}{2}\\left(\\boldsymbol{\\mu}_z'(diag(\\boldsymbol{R}\\sigma^2))^{-1}\\boldsymbol{\\mu}_z\\right),\\\\\n\\boldsymbol{\\mu}_z &= \\boldsymbol{R}\\boldsymbol{Z},\\\\\n\\boldsymbol{R} &= \\boldsymbol{I}_n - \\boldsymbol{X}(\\boldsymbol{X}'\\boldsymbol{X})^{-1}\\boldsymbol{X}',\n\\end{align*}\n\nwhere $diag(.)$ is the diagonal matrix constructed from the diagonal elements of a matrix.\n\n---\n\n## üìèEffect size\n\nAnd the effect size of the **heteroskedasticity model** is\n\n$$E = \\frac{1}{2}\\left(log\\frac{|diag(\\boldsymbol{R}\\boldsymbol{V}\\boldsymbol{R}')|}{|diag(\\boldsymbol{R})|} - n + tr(diag(\\boldsymbol{R}\\boldsymbol{V}\\boldsymbol{R}')^{-1}diag(\\boldsymbol{R}))\\right),$$\nwhere $\\boldsymbol{V}$ is the actual covariance matrix of the error term.\n\n---\n\n## üí™Power of Visual Tests\n\nWe use the **logistic regression** to estimate the power:\n\n$$Pr(\\text{reject}~H_0|H_1,E) = \\Lambda\\left(log\\left(\\frac{0.05}{0.95}\\right) + \\beta_1 E\\right),$$\n\nwhere $\\Lambda(.)$ is the standard logistic function given as $\\Lambda(z) = exp(z)/(1+exp(z))$. \n\n- The **effect size $E$** is the only predictor.\n\n- The intercept is fixed to $log(0.05/0.95)$ so that $\\hat{Pr}(\\text{reject}~H_0|H_1,E = 0) = 0.05$.\n\n---\n\n## üõ†Ô∏èExperimental Setup\n\nWe recruited 443 subjects from an crowd-sourcing platform called **Prolific** (Palan and Schitter, 2018). \nEvery subject will be asked to:\n\n- Evaluate **a block of 20 lineups one by one**.\n- Select **one or more** plots that are **most different** from others.\n- Provide **a reason** for their selections.\n- Evaluate **how different** they think the selected plots are from others. \n\n---\n\n## üõ†Ô∏èExperimental Setup\n\n::: {.fragment}\n\nIf there is **no noticeable difference** between plots in a lineup, subjects are permitted to **select zero plots** without providing the reason.\n\n:::\n\n\n::: {.fragment}\n\nA subject‚Äôs submission is only accepted if the data plot is identified for **at least one attention check**.\n\n:::\n\n\n::: {.fragment}\n\nOverall, we collected **7974 evaluations** on **1152 unique lineups** performed by **443 subjects** throughout three data collection periods.\n\n:::\n\n---\n\n## üåêStudy Website\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](figures/lineup1.png){fig-align='center' width=1679}\n:::\n:::\n\n\n---\n\n## ‚öñÔ∏èResults: Power Comparison of Conventional Tests and Visual Tests {.center}\n\n---\n\n## ‚öñÔ∏èNon-linearity Patterns\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](figures/polypower-1.png){fig-align='center' width=1125}\n:::\n:::\n\n\n---\n\n## ‚öñÔ∏èHeteroskedasticity Patterns\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](figures/heterpower-1.png){fig-align='center' width=1125}\n:::\n:::\n\n\n\n---\n\n\nThe visual test rejects **less frequently** than the conventional test, and (almost) **only rejects when the conventional test does**.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](2023_OCT_26_TALK_files/figure-revealjs/unnamed-chunk-11-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n---\n\n## üåüAn example of conventional tests being too sensitive\n\n::: {style=\"font-size:80%\"}\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.fragment}\nThe data plot (No.1) is **undistinguishable** from other plots with an extremely small effect size (\\\\(log_e(E) = -0.48\\\\)).\n:::\n::: {.fragment}\nThe non-linearity pattern is **totally undetectable**. \n:::\n::: {.fragment}\nHowever, the RESET test rejects the pattern with a very small $p\\text{-value} = 0.004$. In contrast, the $p\\text{-value}$ produced by the visual test is $0.813$.\n:::\n:::\n\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](figures/230.png){width=1050}\n:::\n:::\n\n:::\n\n::::\n\n:::\n\n---\n\n## ‚úÖAdvantages of visual tests in regression diagnostics\n\n- Do not require specifying the pattern **ahead of time**.\n- Rely purely on **whether the data plot is distinguishable** from \"good\" residual plots.\n- Perform **equally well** regardless of the type of residual departures.\n- **Remove any subjective arguments** about whether a pattern is visible or not.\n\n---\n\n## ‚öñÔ∏èResults: Other Interesting Findings {.center}\n\n---\n\nThe default RESET tests **under-performs significantly** in detecting the \"triple-U\" shape.\n\nTo achieve a similar power as other shapes, a **higher order polynomial parameter** needs to be used for the RESET test, but this is higher than the recommended value (4).\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](2023_OCT_26_TALK_files/figure-revealjs/unnamed-chunk-13-1.png){fig-align='center' width=1200}\n:::\n:::\n\n\n---\n\nThe butterfly shape has higher power in both tests.\n\nCuriously, the visual test has **slightly higher power** for the \"left-triangle\" than the \"right-triangle\" shape.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](2023_OCT_26_TALK_files/figure-revealjs/unnamed-chunk-14-1.png){fig-align='center' width=1200}\n:::\n:::\n\n\n---\n\n::: {style=\"font-size:80%\"}\nSurprisingly, the fitted value distribution has produces **more variability** in the power of conventional tests than visual tests.\n\n**Uneven distributions** (normal and lognormal distributions) tend to yield lower power.\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](figures/different-x-dist-poly-power-1.png){fig-align='center' width=1800}\n:::\n:::\n\n\n\n---\n\n## üßêLimitation and Practicality\n\n\nFor this study, we used simulated data and only focused on the **most commonly used**, residual vs fitted value plots.\n\nHowever, we expect the behavior of the conventional test and the visual test to be **similar** when observed residuals are diagnosed with this type of plot or other residual plots.\n\n\n\n---\n\n## üßêMain Conclusions\n\n- Conventional residual-based statistical tests are **more sensitive to weak departures** from model assumptions than visual tests as would be evaluated by humans.\n\n- Conventional tests often reject when departures in the form of non-linearity and heteroskedasticity are **not visibly different** from null residual plots.\n\n- Regression experts are right. Residual plots are **indispensable methods** for assessing model fit.\n\n---\n\n## Thanks! Any questions? {.center}\n\n\n",
    "supporting": [
      "2023_OCT_26_TALK_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}